{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "# SETRED\n",
    "import sys,os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from setred_package import setred_scratch, simulated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Simulate unlabeling 30% of the samples\n",
    "rng = np.random.RandomState(42)\n",
    "mask_unlabeled = rng.rand(len(y)) < 0.3\n",
    "y[mask_unlabeled] = -1  # unlabeled samples\n",
    "\n",
    "# Ensure input arrays are valid and safe to process\n",
    "X = check_array(X)\n",
    "y = check_array(y, ensure_2d=False, dtype=y.dtype.type)\n",
    "\n",
    "# Separate labeled and unlabeled samples\n",
    "X_label = X[y != y.dtype.type(-1)]\n",
    "y_label = y[y != y.dtype.type(-1)]\n",
    "X_unlabel = X[y == y.dtype.type(-1)]\n",
    "\n",
    "# Print results\n",
    "print(f\"Total samples: {len(y)}\")\n",
    "print(f\"Labeled samples: {len(y_label)}\")\n",
    "print(f\"Unlabeled samples: {len(X_unlabel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import check_X_y\n",
    "\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "y = [0, 1, 0]\n",
    "\n",
    "X_checked, y_checked = check_X_y(X, y)\n",
    "\n",
    "print(X_checked.shape)  # (3, 2)\n",
    "print(y_checked.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pre_yL = np.array([0, 1, 2, 1])\n",
    "\n",
    "mismatch_matrix = (pre_yL[:, None] != pre_yL[None, :]).astype(int)\n",
    "\n",
    "print(mismatch_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_df = pd.DataFrame(X_checked, columns=['feature1', 'feature2'])\n",
    "isinstance(X_df, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calculator:\n",
    "    def __init__(self, a, b):\n",
    "        \"\"\"Initialize the calculator with two numbers.\"\"\"\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def add(self):\n",
    "        \"\"\"Return the sum of the two stored numbers.\"\"\"\n",
    "        return self.a + self.b\n",
    "\n",
    "    def subtract(self):\n",
    "        \"\"\"Return the result of subtracting the second number from the first.\"\"\"\n",
    "        return self.a - self.b\n",
    "\n",
    "# Example usage\n",
    "calc = Calculator(10, 5)\n",
    "\n",
    "print(calc.add())      # Output: 15\n",
    "print(calc.subtract()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior_probability(y):\n",
    "    \"\"\"Calculate the priori probability of each label\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like of shape (n_samples,)\n",
    "        array of labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    class_probability: dict\n",
    "        dictionary with priori probability (value) of each label (key)\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    u_c = dict(zip(unique, counts))\n",
    "    instances = len(y)\n",
    "    for u in u_c:\n",
    "        u_c[u] = float(u_c[u] / instances)\n",
    "    return u_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array with 100 values between 0 and 2\n",
    "y = np.random.choice([0, 1, 2], size=150)\n",
    "y_probabilities = calculate_prior_probability(y)\n",
    "y_ = np.random.choice([0, 1, 2], size=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(list(y_probabilities.keys()))\n",
    "sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.searchsorted(np.array(list(y_probabilities.keys())), y_, sorter = sort_idx )\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wrong = 1 - np.asarray(np.array(list(y_probabilities.values())))[sort_idx][idx]\n",
    "p_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.repeat(p_wrong, weights.shape[1]).shape[0]/y_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "random_state = np.random.RandomState(42)\n",
    "iid_random = random_state.binomial(\n",
    "                1, np.repeat(p_wrong, weights.shape[1]).reshape(weights.shape)\n",
    "            )\n",
    "np.repeat(p_wrong, weights.shape[1]).reshape(weights.shape)[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Train a base classifier\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Let's simulate U_ as 3 new samples from the dataset\n",
    "U_ = X[50:54]  # Shape: (3, 4)\n",
    "\n",
    "# Apply your code manually\n",
    "raw_predictions = clf.predict_proba(U_)\n",
    "predictions = np.max(raw_predictions, axis=1)       # Highest class probability for each sample\n",
    "class_predicted = np.argmax(raw_predictions, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(raw_predictions, axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(raw_predictions, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_variables = np.random.rand(100, 4)\n",
    "predictions = np.max(random_variables, axis=1)\n",
    "class_predicted = np.argmax(random_variables, axis=1)\n",
    "indexes = predictions.argsort()[-10:]  # Get indices of the top 3 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_ = np.array(['A', 'B', 'C'])\n",
    "class_predicted = np.array([2, 0, 1])  # Class indices\n",
    "indexes = [0, 2]                       # We're interested in the first and third\n",
    "\n",
    "# class_predicted[indexes] → [2, 1]\n",
    "# classes_[2] → 'C', classes_[1] → 'B'\n",
    "\n",
    "# Result:\n",
    "y_ = np.array(['C', 'B'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions.shape\n",
    "new_instance = np.array([1,2,3])\n",
    "new_instance = new_instance.reshape(1,3)\n",
    "np.concatenate((raw_predictions,new_instance), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Build kneighbors graph\n",
    "A = kneighbors_graph(X, n_neighbors=3, mode='distance', include_self=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to a NetworkX graph\n",
    "G = nx.from_scipy_sparse_array(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: use a layout (e.g., spring layout)\n",
    "pos = nx.spring_layout(G, seed=42)  # Optional: reproducible layout\n",
    "\n",
    "# Option 2: use actual data for positions (e.g., first two features)\n",
    "# pos = {i: X[i, :2] for i in range(X.shape[0])}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(G, pos, node_size=50, node_color=y, cmap=plt.cm.viridis, with_labels=False)\n",
    "plt.title(\"k-Nearest Neighbors Graph (k=3)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = A.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iweights = np.divide(1,weights, out=np.zeros_like(weights), where= weights!=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[weights != 0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use fixed seed for reproducibility\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# 10 coin tosses with 30% chance of heads (1)\n",
    "samples = rng.binomial(n=1, p=np.array([0.3,0.7]), size=10)\n",
    "\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pwrong = np.random.rand(5)\n",
    "weights = weights[-30:,:]\n",
    "pwrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.searchsorted(np.array(list(y_probabilities.keys())), y_, sorter = sort_idx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Setred(BaseEstimator, MetaEstimatorMixin):\n",
      "    \"\"\"\n",
      "    **Self-training with Editing.**\n",
      "    ----------------------------\n",
      "\n",
      "    Create a SETRED classifier. It is a self-training algorithm that uses a rejection mechanism to avoid adding noisy samples to the training set.\n",
      "    The main process are:\n",
      "    1. Train a classifier with the labeled data.\n",
      "    2. Create a pool of unlabeled data and select the most confident predictions.\n",
      "    3. Repeat until the maximum number of iterations is reached:\n",
      "        a. Select the most confident predictions from the unlabeled data.\n",
      "        b. Calculate the neighborhood graph of the labeled data and the selected instances from the unlabeled data.\n",
      "        c. Calculate the significance level of the selected instances.\n",
      "        d. Reject the instances that are not significant according their position in the neighborhood graph.\n",
      "        e. Add the selected instances to the labeled data and retrains the classifier.\n",
      "        f. Add new instances to the pool of unlabeled data.\n",
      "    4. Return the classifier trained with the labeled data.\n",
      "\n",
      "    **Example**\n",
      "    -----------\n",
      "    ```python\n",
      "    from sklearn.datasets import load_iris\n",
      "    from sslearn.model_selection import artificial_ssl_dataset\n",
      "    from sslearn.wrapper import Setred\n",
      "\n",
      "    X, y = load_iris(return_X_y=True)\n",
      "    X, y, X_unlabel, y_unlabel, _, _ = artificial_ssl_dataset(X, y, label_rate=0.1, random_state=0)\n",
      "\n",
      "    clf = Setred()\n",
      "    clf.fit(X, y)\n",
      "    clf.score(X_unlabel, y_unlabel)\n",
      "    ```\n",
      "\n",
      "    **References**\n",
      "    ----------\n",
      "    Li, Ming, and Zhi-Hua Zhou. (2005)<br>\n",
      "    SETRED: Self-training with editing,<br>\n",
      "    in <i>Advances in Knowledge Discovery and Data Mining.</i> <br>\n",
      "    Pacific-Asia Conference on Knowledge Discovery and Data Mining <br>\n",
      "    LNAI 3518, Springer, Berlin, Heidelberg, <br>\n",
      "    [10.1007/11430919_71](https://doi.org/10.1007/11430919_71)\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        base_estimator=KNeighborsClassifier(n_neighbors=3),\n",
      "        max_iterations=40,\n",
      "        distance=\"euclidean\",\n",
      "        poolsize=0.25,\n",
      "        rejection_threshold=0.05,\n",
      "        graph_neighbors=1,\n",
      "        random_state=None,\n",
      "        n_jobs=None,\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Create a SETRED classifier.\n",
      "        It is a self-training algorithm that uses a rejection mechanism to avoid adding noisy samples to the training set.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        base_estimator : ClassifierMixin, optional\n",
      "            An estimator object implementing fit and predict_proba, by default KNeighborsClassifier(n_neighbors=3)\n",
      "        max_iterations : int, optional\n",
      "            Maximum number of iterations allowed. Should be greater than or equal to 0., by default 40\n",
      "        distance : str, optional\n",
      "            The distance metric to use for the graph.\n",
      "            The default metric is euclidean, and with p=2 is equivalent to the standard Euclidean metric.\n",
      "            For a list of available metrics, see the documentation of DistanceMetric and the metrics listed in sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            Note that the `cosine` metric uses cosine_distances., by default `euclidean`\n",
      "        poolsize : float, optional\n",
      "            Max number of unlabel instances candidates to pseudolabel, by default 0.25\n",
      "        rejection_threshold : float, optional\n",
      "            significance level, by default 0.05\n",
      "        graph_neighbors : int, optional\n",
      "            Number of neighbors for each sample., by default 1\n",
      "        random_state : int, RandomState instance, optional\n",
      "            controls the randomness of the estimator, by default None\n",
      "        n_jobs : int, optional\n",
      "            The number of parallel jobs to run for neighbors search. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors, by default None\n",
      "        \"\"\"\n",
      "        self.base_estimator = check_classifier(base_estimator, can_be_list=False)\n",
      "        self.max_iterations = max_iterations\n",
      "        self.poolsize = poolsize\n",
      "        self.distance = distance\n",
      "        self.rejection_threshold = rejection_threshold\n",
      "        self.graph_neighbors = graph_neighbors\n",
      "        self.random_state = random_state\n",
      "        self.n_jobs = n_jobs\n",
      "\n",
      "    def __create_neighborhood(self, X):\n",
      "        # kneighbors_graph(X, 1, metric=self.distance, n_jobs=self.n_jobs).toarray()\n",
      "        return kneighbors_graph(\n",
      "            X, self.graph_neighbors, metric=self.distance, n_jobs=self.n_jobs, mode=\"distance\"\n",
      "        ).toarray()\n",
      "\n",
      "    def fit(self, X, y, **kwars):\n",
      "        \"\"\"Build a Setred classifier from the training set (X, y).\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The training input samples.\n",
      "        y : array-like of shape (n_samples,)\n",
      "            The target values (class labels), -1 if unlabeled.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        self: Setred\n",
      "            Fitted estimator.\n",
      "        \"\"\"        \n",
      "        random_state = check_random_state(self.random_state)\n",
      "\n",
      "        X_label, y_label, X_unlabel = get_dataset(X, y)\n",
      "\n",
      "        is_df = isinstance(X_label, pd.DataFrame)\n",
      "\n",
      "        self.classes_ = np.unique(y_label)\n",
      "\n",
      "        each_iteration_candidates = X_label.shape[0]\n",
      "\n",
      "        pool = int(len(X_unlabel) * self.poolsize)\n",
      "        self._base_estimator = skclone(self.base_estimator)\n",
      "\n",
      "        self._base_estimator.fit(X_label, y_label, **kwars)\n",
      "\n",
      "        y_probabilities = calculate_prior_probability(\n",
      "            y_label\n",
      "        )  # Should probabilities change every iteration or may it keep with the first L?\n",
      "\n",
      "        sort_idx = np.argsort(list(y_probabilities.keys()))\n",
      "\n",
      "        if X_unlabel.shape[0] == 0:\n",
      "            return self\n",
      "\n",
      "        for _ in range(self.max_iterations):\n",
      "            U_ = resample(\n",
      "                X_unlabel, replace=False, n_samples=pool, random_state=random_state\n",
      "            )\n",
      "\n",
      "            if is_df:\n",
      "                U_ = pd.DataFrame(U_, columns=X_label.columns)\n",
      "\n",
      "            raw_predictions = self._base_estimator.predict_proba(U_)\n",
      "            predictions = np.max(raw_predictions, axis=1)\n",
      "            class_predicted = np.argmax(raw_predictions, axis=1)\n",
      "            # Unless a better understanding is given, only the size of L will be used as maximal size of the candidate set.\n",
      "            indexes = predictions.argsort()[-each_iteration_candidates:]\n",
      "\n",
      "            if is_df:\n",
      "                L_ = U_.iloc[indexes]\n",
      "            else:\n",
      "                L_ = U_[indexes]\n",
      "            y_ = np.array(\n",
      "                list(\n",
      "                    map(\n",
      "                        lambda x: self._base_estimator.classes_[x],\n",
      "                        class_predicted[indexes],\n",
      "                    )\n",
      "                )\n",
      "            )\n",
      "\n",
      "            if is_df:\n",
      "                pre_L = pd.concat([X_label, L_])\n",
      "            else:\n",
      "                pre_L = np.concatenate((X_label, L_), axis=0)\n",
      "\n",
      "            weights = self.__create_neighborhood(pre_L)\n",
      "            #  Keep only weights for L_\n",
      "            weights = weights[-L_.shape[0]:, :]\n",
      "\n",
      "            idx = np.searchsorted(np.array(list(y_probabilities.keys())), y_, sorter=sort_idx)\n",
      "            p_wrong = 1 - np.asarray(np.array(list(y_probabilities.values())))[sort_idx][idx]\n",
      "            #  Must weights be the inverse of distance?\n",
      "            weights = np.divide(1, weights, out=np.zeros_like(weights), where=weights != 0)\n",
      "\n",
      "            weights_sum = weights.sum(axis=1)\n",
      "            weights_square_sum = (weights ** 2).sum(axis=1)\n",
      "\n",
      "            iid_random = random_state.binomial(\n",
      "                1, np.repeat(p_wrong, weights.shape[1]).reshape(weights.shape)\n",
      "            )\n",
      "            ji = (iid_random * weights).sum(axis=1)\n",
      "\n",
      "            mu_h0 = p_wrong * weights_sum\n",
      "            sigma_h0 = np.sqrt((1 - p_wrong) * p_wrong * weights_square_sum)\n",
      "            \n",
      "            z_score = np.divide((ji - mu_h0), sigma_h0, out=np.zeros_like(sigma_h0), where=sigma_h0 != 0)\n",
      "            # z_score = (ji - mu_h0) / sigma_h0\n",
      "            \n",
      "            oi = norm.sf(abs(z_score), mu_h0, sigma_h0)\n",
      "            to_add = (oi < self.rejection_threshold) & (z_score < mu_h0)\n",
      "\n",
      "            if is_df:\n",
      "                L_filtered = L_.iloc[to_add, :]\n",
      "            else:\n",
      "                L_filtered = L_[to_add, :]\n",
      "            y_filtered = y_[to_add]\n",
      "            \n",
      "            if is_df:\n",
      "                X_label = pd.concat([X_label, L_filtered])\n",
      "            else:\n",
      "                X_label = np.concatenate((X_label, L_filtered), axis=0)\n",
      "            y_label = np.concatenate((y_label, y_filtered), axis=0)\n",
      "\n",
      "            #  Remove the instances from the unlabeled set.\n",
      "            to_delete = indexes[to_add]\n",
      "            if is_df:\n",
      "                X_unlabel = X_unlabel.drop(index=X_unlabel.index[to_delete])\n",
      "            else:\n",
      "                X_unlabel = np.delete(X_unlabel, to_delete, axis=0)\n",
      "\n",
      "        return self\n",
      "\n",
      "    def predict(self, X, **kwards):\n",
      "        \"\"\"Predict class value for X.\n",
      "        For a classification model, the predicted class for each sample in X is returned.\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The input samples.\n",
      "        Returns\n",
      "        -------\n",
      "        y : array-like of shape (n_samples,)\n",
      "            The predicted classes\n",
      "        \"\"\"\n",
      "        return self._base_estimator.predict(X, **kwards)\n",
      "\n",
      "    def predict_proba(self, X, **kwards):\n",
      "        \"\"\"Predict class probabilities of the input samples X.\n",
      "        The predicted class probability depends on the ensemble estimator.\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The input samples.\n",
      "        Returns\n",
      "        -------\n",
      "        y : ndarray of shape (n_samples, n_classes) or list of n_outputs such arrays if n_outputs > 1\n",
      "            The predicted classes\n",
      "        \"\"\"\n",
      "        return self._base_estimator.predict_proba(X, **kwards)\n",
      "    \n",
      "    def score(self, X, y, sample_weight=None):\n",
      "        \"\"\"\n",
      "        Return the mean accuracy on the given test data and labels.\n",
      "\n",
      "        In multi-label classification, this is the subset accuracy\n",
      "        which is a harsh metric since you require for each sample that\n",
      "        each label set be correctly predicted.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Test samples.\n",
      "\n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            True labels for `X`.\n",
      "\n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "        \"\"\"\n",
      "        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from sklearn.base import BaseEstimator\n",
    "from sslearn.wrapper import Setred\n",
    "\n",
    "print(inspect.getsource(Setred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Combined class labels\n",
    "pre_yL = np.array([0, 1, 0, 2, 1])\n",
    "print(\"Pre-labeled classes:\", pre_yL)\n",
    "# Construct the class contrast matrix\n",
    "C = (pre_yL[:, None] != pre_yL[None, :]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_yL[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_yL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def simulate_ji_matrix(p_wrong, weights, weights_sum, weights_square_sum, n_simulations, random_state):\n",
    "    n_instances, n_neighbors = weights.shape\n",
    "\n",
    "    # Precompute simulation probabilities\n",
    "    p_matrix = np.repeat(p_wrong, n_neighbors).reshape(weights.shape)\n",
    "\n",
    "    # Matrix to store all simulated ji values\n",
    "    ji_matrix = np.zeros((n_instances,n_simulations))\n",
    "\n",
    "    for s in range(n_simulations):\n",
    "        # Simulate binary decisions\n",
    "        iid_random = random_state.binomial(1, p_matrix)\n",
    "        \n",
    "        # Simulate test statistic\n",
    "        ji = (iid_random * weights).sum(axis=1)\n",
    "        ji_matrix[:, s] = ji\n",
    "\n",
    "        # (Optional for SETRED): Compute p-value and filtering condition\n",
    "        mu_h0 = p_wrong * weights_sum\n",
    "        sigma_h0 = np.sqrt((1 - p_wrong) * p_wrong * weights_square_sum)\n",
    "\n",
    "        z_score = np.divide((ji - mu_h0), sigma_h0, out=np.zeros_like(sigma_h0), where=sigma_h0 != 0)\n",
    "        oi = norm.sf(abs(z_score), mu_h0, sigma_h0)\n",
    "        to_add = (oi < 0.05) & (z_score < mu_h0)  # or use self.rejection_threshold\n",
    "\n",
    "        # If you're using to_add for filtering, handle it externally\n",
    "\n",
    "    return ji_matrix\n",
    "\n",
    "\n",
    "def compare_to_observed(jiobs, ji_matrix):\n",
    "    # Returns a count or proportion of times simulated ji > observed\n",
    "    return 1 - np.mean(jiobs[:,None] < ji_matrix, axis=1)  # count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Step 1: Define your simulated setup\n",
    "random_state = np.random.RandomState(42)\n",
    "n_instances = 5\n",
    "n_neighbors = 4\n",
    "n_simulations = 1000\n",
    "\n",
    "# Example weights between subjects (normally distances)\n",
    "weights = np.random.rand(n_instances, n_neighbors)\n",
    "\n",
    "# Probabilities of being wrong (say from prior distribution)\n",
    "p_wrong = np.random.uniform(0.1, 0.4, size=n_instances)\n",
    "\n",
    "# Precomputed stats used for normalization\n",
    "weights_sum = weights.sum(axis=1)\n",
    "weights_square_sum = (weights ** 2).sum(axis=1)\n",
    "\n",
    "# Step 2: Create observed test statistic\n",
    "# Simulate one Bernoulli draw as if observed\n",
    "p_matrix = np.repeat(p_wrong, n_neighbors).reshape(weights.shape)\n",
    "iid_observed = random_state.binomial(1, p_matrix)\n",
    "ji_obs = (iid_observed * weights).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation and comparison\n",
    "ji_sim = simulate_ji_matrix(p_wrong, weights, weights_sum, weights_square_sum,\n",
    "                            n_simulations=2, random_state=random_state)\n",
    "\n",
    "ji_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ji_obs[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ji_obs[:, None] < ji_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ji_obs[:, None] < ji_sim).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue_simulated = np.mean(jiobs[:, None] < ji_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = compare_to_observed(ji_obs, ji_sim)\n",
    "\n",
    "# Print results\n",
    "print(\"Observed ji:\", ji_obs)\n",
    "print(\"Simulated ji (first 5):\\n\", ji_sim[:5])\n",
    "print(\"Counts where simulated > observed:\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Reference and new sets\n",
    "X_ref = np.array([[0, 0], [1, 1], [2, 2]])      # shape (n_ref, d)\n",
    "X_new = np.array([[1, 0], [3, 3]])              # shape (n_new, d)\n",
    "\n",
    "# Step 1: Fit on reference set\n",
    "k = 2\n",
    "nn = NearestNeighbors(n_neighbors=k)\n",
    "nn.fit(X_ref)\n",
    "\n",
    "# Step 2: Find neighbors of new points in the reference set\n",
    "distances, indices = nn.kneighbors(X_new)\n",
    "\n",
    "# Step 3: Build sparse graph: shape (n_new, n_ref)\n",
    "n_new, n_ref = X_new.shape[0], X_ref.shape[0]\n",
    "rows = np.repeat(np.arange(n_new), k)\n",
    "cols = indices.flatten()\n",
    "data = np.ones_like(rows)\n",
    "data = distances.flatten()\n",
    "# If you want distances instead of connectivity, use `data = distances.flatten()`\n",
    "A = csr_matrix((data, (rows, cols)), shape=(n_new, n_ref))\n",
    "\n",
    "print(A.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros((X_new.shape[0], X_ref.shape[0]))\n",
    "for i, (idx,dist) in enumerate(zip(indices, distances)):\n",
    "    print(f\"New point {i} neighbors (indices): {idx}, distances: {dist}\")\n",
    "    weights[i, idx] = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jfja_dissertation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
